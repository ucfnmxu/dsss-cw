{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"float:left\">\n",
    "    <h1 style=\"width:600px\">Workshop 7: Classification</h1>\n",
    "    <h3 style=\"width:600px\">CASA0006: Data Science for Spatial Systems</h3>\n",
    "    <h3 style=\"width:600px\">Author: Huanfa Chen</h3>\n",
    "</div>\n",
    "<div style=\"float:right\"><img width=\"100\" src=\"https://github.com/jreades/i2p/raw/master/img/casa_logo.jpg\" /></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last executed: 2022-03-04 17:14:13\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "now = datetime.datetime.now()\n",
    "print(\"Last executed: \" + now.strftime(\"%Y-%m-%d %H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This week we will focus on **classification algorithms and applications**.  Classifiers are highly useful in extracting patterns from large datasets. By setting up a classifier on an understood, pre-classified subset of the data, we are able to automatically derive greater understanding into other, unclassified datasets.\n",
    "\n",
    "The process of creating a classifier remains the same regardless of the methods:\n",
    "\n",
    "1. Organise and clean your dataset.\n",
    "2. Divide the dataset into training and testing subsets.\n",
    "3. Use the classifier to associate feature attributes within the training dataset to *known* classifications (known as *training* or *calibration*).\n",
    "4. Test the strength of model fit by predicting the classifications within the test dataset (known as *testing* or *validation*).\n",
    "5. If you are happy with the model performance, you can apply the classifier to any new and future data in order to establish classifications. This provides instant insight into the type of data you are receiving.\n",
    "\n",
    "Once more during this session, we will use Python and specifically the **`pandas`** and **`scikit-learn`** libraries. The method for the application of different classification methods using `sklearn` is helpfully very consistent, as such we will explore a range of different methods today. We will also look at ways to explore the quality of the prediction results, in order to understand how well our mining is going.\n",
    "\n",
    "As usual, we need to import the relevant libraries to get started, so **run the script below** first to give us access to our basic data analysis packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_rows', 300) # specifies number of rows to show\n",
    "pd.options.display.float_format = '{:40,.4f}'.format # specifies default number format to 4 decimal places"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset you'll be working with today relates to the personal characteristics of over 32,000 individuals. Our task is to work out whether we can predict whether an individual's income is above or below $50,000 per annum, based solely on their demographic characteristics. While we are dealing with quite a few attributes here, we're only predicting over two classes, thus it is a relatively straightforward classification problem.\n",
    "\n",
    "This dataset is originally from the UCI Machine Learning Repository in this [link](https://archive.ics.uci.edu/ml/datasets/Adult). It is also known as the \"Census Income\" dataset. We downloaded this dataset and transformed it into a csv file so that it is ready to use.\n",
    "\n",
    "**Download the dataset from Github, and import it as a Pandas dataframe called `original_data`. Inspect the dataset, and use the accompanying metadata to help you understand what you have.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data=pd.read_csv('https://raw.githubusercontent.com/huanfachen/Spatial_Data_Science/main/Dataset/income.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>over50k</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32556</th>\n",
       "      <td>27</td>\n",
       "      <td>Private</td>\n",
       "      <td>257302</td>\n",
       "      <td>Assoc-acdm</td>\n",
       "      <td>12</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Tech-support</td>\n",
       "      <td>Wife</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32557</th>\n",
       "      <td>40</td>\n",
       "      <td>Private</td>\n",
       "      <td>154374</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32558</th>\n",
       "      <td>58</td>\n",
       "      <td>Private</td>\n",
       "      <td>151910</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32559</th>\n",
       "      <td>22</td>\n",
       "      <td>Private</td>\n",
       "      <td>201490</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32560</th>\n",
       "      <td>52</td>\n",
       "      <td>Self-emp-inc</td>\n",
       "      <td>287927</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Wife</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>15024</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32561 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       age         workclass  fnlwgt   education  education-num  \\\n",
       "0       39         State-gov   77516   Bachelors             13   \n",
       "1       50  Self-emp-not-inc   83311   Bachelors             13   \n",
       "2       38           Private  215646     HS-grad              9   \n",
       "3       53           Private  234721        11th              7   \n",
       "4       28           Private  338409   Bachelors             13   \n",
       "...    ...               ...     ...         ...            ...   \n",
       "32556   27           Private  257302  Assoc-acdm             12   \n",
       "32557   40           Private  154374     HS-grad              9   \n",
       "32558   58           Private  151910     HS-grad              9   \n",
       "32559   22           Private  201490     HS-grad              9   \n",
       "32560   52      Self-emp-inc  287927     HS-grad              9   \n",
       "\n",
       "           marital-status         occupation   relationship   race     sex  \\\n",
       "0           Never-married       Adm-clerical  Not-in-family  White    Male   \n",
       "1      Married-civ-spouse    Exec-managerial        Husband  White    Male   \n",
       "2                Divorced  Handlers-cleaners  Not-in-family  White    Male   \n",
       "3      Married-civ-spouse  Handlers-cleaners        Husband  Black    Male   \n",
       "4      Married-civ-spouse     Prof-specialty           Wife  Black  Female   \n",
       "...                   ...                ...            ...    ...     ...   \n",
       "32556  Married-civ-spouse       Tech-support           Wife  White  Female   \n",
       "32557  Married-civ-spouse  Machine-op-inspct        Husband  White    Male   \n",
       "32558             Widowed       Adm-clerical      Unmarried  White  Female   \n",
       "32559       Never-married       Adm-clerical      Own-child  White    Male   \n",
       "32560  Married-civ-spouse    Exec-managerial           Wife  White  Female   \n",
       "\n",
       "       capital-gain  capital-loss  hours-per-week native-country over50k  \n",
       "0              2174             0              40  United-States   <=50K  \n",
       "1                 0             0              13  United-States   <=50K  \n",
       "2                 0             0              40  United-States   <=50K  \n",
       "3                 0             0              40  United-States   <=50K  \n",
       "4                 0             0              40           Cuba   <=50K  \n",
       "...             ...           ...             ...            ...     ...  \n",
       "32556             0             0              38  United-States   <=50K  \n",
       "32557             0             0              40  United-States    >50K  \n",
       "32558             0             0              40  United-States   <=50K  \n",
       "32559             0             0              20  United-States   <=50K  \n",
       "32560         15024             0              40  United-States    >50K  \n",
       "\n",
       "[32561 rows x 15 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 32561 entries, 0 to 32560\n",
      "Data columns (total 15 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   age             32561 non-null  int64 \n",
      " 1   workclass       32561 non-null  object\n",
      " 2   fnlwgt          32561 non-null  int64 \n",
      " 3   education       32561 non-null  object\n",
      " 4   education-num   32561 non-null  int64 \n",
      " 5   marital-status  32561 non-null  object\n",
      " 6   occupation      32561 non-null  object\n",
      " 7   relationship    32561 non-null  object\n",
      " 8   race            32561 non-null  object\n",
      " 9   sex             32561 non-null  object\n",
      " 10  capital-gain    32561 non-null  int64 \n",
      " 11  capital-loss    32561 non-null  int64 \n",
      " 12  hours-per-week  32561 non-null  int64 \n",
      " 13  native-country  32561 non-null  object\n",
      " 14  over50k         32561 non-null  object\n",
      "dtypes: int64(6), object(9)\n",
      "memory usage: 3.7+ MB\n"
     ]
    }
   ],
   "source": [
    "original_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you will notice, the data you have been provided with an indicator stating whether the individual does or does not earn over $50000 per annum. Well we use this indicator to define the relationship between the individual's socioeconomic characteristics and this classification.\n",
    "\n",
    "But to get there, we need to start splitting up our dataset. We need one dataset containing the independent variables, and we need another dataset containing our classifications. We'll eventually pass both of these to the classifier so that it can identify the classification relationship.\n",
    "\n",
    "So first, we will create our attribute dataset. **Create a new dataframe called `data` that includes all attributes from `original_data` except the `over50k` attribute. Remember to check the contents of your new dataframe to ensure this is the case.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=original_data.iloc[:, :14]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create the label dataset (the `y` variable) by transforming the `over50k` variable into numerics. To do this, we'll use the `LabelEncoder`. This function takes a column of target labels and encode target labels with value between 0 and `n_classes - 1`. A documentation of this function is [here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html).\n",
    "\n",
    "We can later use the same encoder to reverse the encoding.\n",
    "\n",
    "**Run the scripts below to first import the `LabelEncoder` tools, and then create the labels dataset from the `over50k` class column.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder() # creates the LabelEncoder object\n",
    "le.fit(['<=50K', '>50K']) # we explicitly encode '<=50k' and '>50k' with 0 and 1, respectively\n",
    "label_y = le.transform(original_data['over50k']) # runs LabelEncoder on the over50k column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the target variable is ready to go. Next, we need to preprocess the X variables and convert them to numericals, as the `sklearn` classification methods do not play well with text variables.\n",
    "\n",
    "We do this by converting each categorical attribute into a range of additional boolean columns representing each category, marked with 1s or 0s. This does not change the structure of the dataset as the boolean values will continue to to distinguish between features.\n",
    "\n",
    "In this case, we use `DictVectorizer`. `DictVectorizer` takes our data as a series of dictionaries, and transforms it into a matrix which is free of categorical data. This function is similar to the `pandas.get_dummies`, although some differences exist. For example, `DictVectorizer` can be integrated into a pipeline in `sklearn` which simplifies the model building, but `pandas.get_dummies` cannot be used with a `sklearn` pipeline.\n",
    "\n",
    "**Run through the following commands.** Starting with importing the `DictVectorizer` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we convert our attribute data to an array of dictionaries for the `DictVectorizer`. We use the `Pandas.to_dict()` function for this. The `'records'` flag ensures that the attribute dataset is converted into an array of dictionaries, where each dictionary represents a single data record.\n",
    "\n",
    "**Run the script to create the dictionaries, and verify the data by checking the first entry.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = data.to_dict('records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now we run the `DictVectorizer` to extract the matrix.** This is a very simple procedure, and note below how the commands needed to run this are very similar to those used earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = DictVectorizer()  # create the DictVectorizer object\n",
    "vec_array = vec.fit_transform(data_dict).toarray()  # execute process on the record dictionaries and transform the result into a numpy array object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now check `vec_array` to see how widely the dataset has been expanded (remember it consisted of 14 attributes before).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.9000e+01, 2.1740e+03, 0.0000e+00, ..., 0.0000e+00, 1.0000e+00,\n",
       "        0.0000e+00],\n",
       "       [5.0000e+01, 0.0000e+00, 0.0000e+00, ..., 1.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00],\n",
       "       [3.8000e+01, 0.0000e+00, 0.0000e+00, ..., 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00],\n",
       "       ...,\n",
       "       [5.8000e+01, 0.0000e+00, 0.0000e+00, ..., 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00],\n",
       "       [2.2000e+01, 0.0000e+00, 0.0000e+00, ..., 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00],\n",
       "       [5.2000e+01, 1.5024e+04, 0.0000e+00, ..., 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of variables in this transformed data: 108\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of variables in this transformed data: {}\".format(vec_array.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can view the feature names by calling the `get_feature_names` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age',\n",
       " 'capital-gain',\n",
       " 'capital-loss',\n",
       " 'education-num',\n",
       " 'education=10th',\n",
       " 'education=11th',\n",
       " 'education=12th',\n",
       " 'education=1st-4th',\n",
       " 'education=5th-6th',\n",
       " 'education=7th-8th',\n",
       " 'education=9th',\n",
       " 'education=Assoc-acdm',\n",
       " 'education=Assoc-voc',\n",
       " 'education=Bachelors',\n",
       " 'education=Doctorate',\n",
       " 'education=HS-grad',\n",
       " 'education=Masters',\n",
       " 'education=Preschool',\n",
       " 'education=Prof-school',\n",
       " 'education=Some-college',\n",
       " 'fnlwgt',\n",
       " 'hours-per-week',\n",
       " 'marital-status=Divorced',\n",
       " 'marital-status=Married-AF-spouse',\n",
       " 'marital-status=Married-civ-spouse',\n",
       " 'marital-status=Married-spouse-absent',\n",
       " 'marital-status=Never-married',\n",
       " 'marital-status=Separated',\n",
       " 'marital-status=Widowed',\n",
       " 'native-country=?',\n",
       " 'native-country=Cambodia',\n",
       " 'native-country=Canada',\n",
       " 'native-country=China',\n",
       " 'native-country=Columbia',\n",
       " 'native-country=Cuba',\n",
       " 'native-country=Dominican-Republic',\n",
       " 'native-country=Ecuador',\n",
       " 'native-country=El-Salvador',\n",
       " 'native-country=England',\n",
       " 'native-country=France',\n",
       " 'native-country=Germany',\n",
       " 'native-country=Greece',\n",
       " 'native-country=Guatemala',\n",
       " 'native-country=Haiti',\n",
       " 'native-country=Holand-Netherlands',\n",
       " 'native-country=Honduras',\n",
       " 'native-country=Hong',\n",
       " 'native-country=Hungary',\n",
       " 'native-country=India',\n",
       " 'native-country=Iran',\n",
       " 'native-country=Ireland',\n",
       " 'native-country=Italy',\n",
       " 'native-country=Jamaica',\n",
       " 'native-country=Japan',\n",
       " 'native-country=Laos',\n",
       " 'native-country=Mexico',\n",
       " 'native-country=Nicaragua',\n",
       " 'native-country=Outlying-US(Guam-USVI-etc)',\n",
       " 'native-country=Peru',\n",
       " 'native-country=Philippines',\n",
       " 'native-country=Poland',\n",
       " 'native-country=Portugal',\n",
       " 'native-country=Puerto-Rico',\n",
       " 'native-country=Scotland',\n",
       " 'native-country=South',\n",
       " 'native-country=Taiwan',\n",
       " 'native-country=Thailand',\n",
       " 'native-country=Trinadad&Tobago',\n",
       " 'native-country=United-States',\n",
       " 'native-country=Vietnam',\n",
       " 'native-country=Yugoslavia',\n",
       " 'occupation=?',\n",
       " 'occupation=Adm-clerical',\n",
       " 'occupation=Armed-Forces',\n",
       " 'occupation=Craft-repair',\n",
       " 'occupation=Exec-managerial',\n",
       " 'occupation=Farming-fishing',\n",
       " 'occupation=Handlers-cleaners',\n",
       " 'occupation=Machine-op-inspct',\n",
       " 'occupation=Other-service',\n",
       " 'occupation=Priv-house-serv',\n",
       " 'occupation=Prof-specialty',\n",
       " 'occupation=Protective-serv',\n",
       " 'occupation=Sales',\n",
       " 'occupation=Tech-support',\n",
       " 'occupation=Transport-moving',\n",
       " 'race=Amer-Indian-Eskimo',\n",
       " 'race=Asian-Pac-Islander',\n",
       " 'race=Black',\n",
       " 'race=Other',\n",
       " 'race=White',\n",
       " 'relationship=Husband',\n",
       " 'relationship=Not-in-family',\n",
       " 'relationship=Other-relative',\n",
       " 'relationship=Own-child',\n",
       " 'relationship=Unmarried',\n",
       " 'relationship=Wife',\n",
       " 'sex=Female',\n",
       " 'sex=Male',\n",
       " 'workclass=?',\n",
       " 'workclass=Federal-gov',\n",
       " 'workclass=Local-gov',\n",
       " 'workclass=Never-worked',\n",
       " 'workclass=Private',\n",
       " 'workclass=Self-emp-inc',\n",
       " 'workclass=Self-emp-not-inc',\n",
       " 'workclass=State-gov',\n",
       " 'workclass=Without-pay']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final stage in data processing involves splitting the prepared dataset into training and testing subsets. The training data will be used to create the classifier. The testing data will then be used to test the accuracy of a the classification. \n",
    "\n",
    "**IMPORTANT**: Splitting the dataset up in this way is an important step. It tests how well the classifier performs against unseen data. This gives a good picture of how well the classifier may work in future. \n",
    "\n",
    "Once more, we have a useful tool is `scikit` to do this. The `train_test_split` method randomly splits our attribute and label data into training and testing subsets. Not only does this provide us with  formats to be loaded directly into our classifers, but the random split ensures a good mixing of the records. \n",
    "\n",
    "The process is quite straightforward. **We load in the `train_test_split` method and run it against our attribute and label arrays.**\n",
    "\n",
    "Note: `train_test_split` will split the data according to a 75:25 split, roughly in line with convention. However, other proportions can be specified, check out the [documentation](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.train_test_split.html) for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state_split = 1024\n",
    "train_d, test_d, train_lab, test_lab = train_test_split(vec_array, label_y, random_state=random_state_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check the lengths of the training and testing datasets.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24420, 8141, 24420, 8141)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_d),len(test_d),len(train_lab),len(test_lab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to construct our classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Artificial Neural Networks (ANN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/astro-informatics/course_mlbd_images/master/Lecture10_Images/ann_layers_multiclass_classification.jpg\" width=\"500px\"/>\n",
    "\n",
    "[Image Credit](https://raw.githubusercontent.com/astro-informatics/course_mlbd_images/master/Lecture10_Images/ann_layers_multiclass_classification.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first classifier will use is the ANN classifier. We will use the **MLPClassifier**, which implements a multi-layer perceptron (MLP) algorithm that trains using backpropagation. Backpropagation is one way of training the ANN model.\n",
    "\n",
    "We will show you how to use the MLPClassifier classifer in `scikit`, as well as going into details about how you can assess the quality of the classification.\n",
    "\n",
    "**IMPORTANT**: Many of the `scikit` classification methods use a very similar syntax. We will show you how to use Logistic Regression, but you will have to work out how to use the others. On completion, we expect you to be able to say which of the four classifiers you will test performs best in predicting whether the individuals in our test dataset earn more or less than $50k per annum.\n",
    "\n",
    "Across all `scikit` classifiers, a similar process and form of syntax is used. \n",
    "\n",
    "1. First, we load the library. \n",
    "2. Then, we create the classifier object, and specify any important parameters.\n",
    "3. We run the `.fit()` method, sending the classifier our training dataset and accompanying labels.\n",
    "4. We analyse the validate with the `.score()` method, sending the classifier our testing dataset and accompanying labels.\n",
    "\n",
    "Let's work through this method for the MLPClassifier classifer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[This page](https://scikit-learn.org/stable/modules/neural_networks_supervised.html#classification) is a general description of the method, and [this page](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html) outlines all of the parameters and methods associated with the `MLPClassifier` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some important settings of this `MLPClassifier` method:\n",
    "\n",
    "- `hidden_layer_sizes: tuple, length = n_layers - 2, default=(100,)`: this argument is a tuple of number of neurons in each hidden layer. By default, the ANN has one hidden layer with 100 neurons.\n",
    "- `activation: {‘identity’, ‘logistic’, ‘tanh’, ‘relu’}, default=’relu’`: this argument is the activation function and can be one of the four functions. By default, the ANN uses the ReLu function, which is the rectified linear unit function, returns f(x) = max(0, x).\n",
    "- `random_state`: Determines random number generation for weights and bias initialization, train-test split, etc. Pass an int for reproducible results across multiple function calls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common activation functions (including relu) is listed as below:\n",
    "\n",
    "Step\n",
    "\n",
    "$$ a(z) = \\left \\{\n",
    "\\begin{eqnarray}\n",
    "0,\\ \\text{if}\\ z < 0\\\\\n",
    "1,\\ \\text{if}\\ z \\geq 0 \\\\\n",
    "\\end{eqnarray}\n",
    "\\right. \n",
    "$$\n",
    "\n",
    "Sigmoid\n",
    "$\n",
    "a(z) = \\frac{1}{1+\\exp{(-z)}}\n",
    "$\n",
    "\n",
    "Hyperboic tangent\n",
    "$\n",
    "a(z) = \\tanh(z)\n",
    "$\n",
    "\n",
    "Rectified linear unit (ReLU)\n",
    "$\n",
    "a(z) = \\max(0, z)\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/astro-informatics/course_mlbd_images/master/Lecture10_Images/activation_func.png\" width=\"500px\"/>\n",
    "\n",
    "[Image Credit](https://raw.githubusercontent.com/astro-informatics/course_mlbd_images/master/Lecture10_Images/activation_func.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You can use the `%%time` magic to time how long the `.fit()` takes to execute.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12min 20s, sys: 47min 42s, total: 1h 2s\n",
      "Wall time: 3min 45s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(random_state=100)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "random_state_ann = 100\n",
    "# add timing script\n",
    "ann_clf = MLPClassifier(random_state = random_state_ann)  # creates the ANN classifier using the default parameters\n",
    "ann_clf.fit(train_d, train_lab)  # executes the classifier on the training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we move to evaluate the model performance, it is good to ask a few questions about the structure of this ANN:\n",
    "\n",
    "1. How many hidden layers are there? (Answer: 1)\n",
    "1. How many neurons are there is each layer (including input/hidden/output layer)? (The input layer has 108 neurons - same as #variables in train_d. Output layer has 2 neurons - same as #class in y)\n",
    "1. Does this classifier tell the probability distribution over the two income levels? In other words, is this class a **soft classifier**? (ANN is a soft classifier as it can ouput the probability overall the y classes.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this classifier to make prediction on one or multiple records.\n",
    "\n",
    "The output can be either the most likely class or the probability distribution over clases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted class of the first individual: [0]\n",
      "The predicted probability of the first individual: [[1.00000000e+00 2.85452201e-59]]\n"
     ]
    }
   ],
   "source": [
    "# to predict the most likely class\n",
    "print(\"The predicted class of the first individual: {}\".format(ann_clf.predict(train_d[0:1, :])))\n",
    "\n",
    "# to predict the probability distribution over classes \n",
    "# the output is a list of two values, which corresponds to the probability of belonging to Class 0 and 1\n",
    "print(\"The predicted probability of the first individual: {}\".format(ann_clf.predict_proba(train_d[0:1, :])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, finally, we test how well the classifier predicts the classes of our test dataset. For this stage, we send the test data and labels to the `.score()` function. This outputs the predictive accuracy. It requires you to pass it both the test data and test labels.\n",
    "\n",
    "**Run the script below to see the accuracy on the testing data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.819432502149613"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann_clf.score(test_d, test_lab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well done, you've created and tested a classifer. And the classifier seems to do pretty well. In any future event, given a load more data we'll be able to predict the salary class of an individual with an expected accuracy of 81.9%, right? \n",
    "\n",
    "Well, hold on there, we better do some further tests just to check how good this score actually is..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of other important ways to assess the quality of your classifier that you should be aware of.\n",
    "\n",
    "First, you can have a look yourself at the actual results, and identify the records where predictions succeed and fail. You can do this by generating a set of predictions for each record, done using the `.predict()` method. Like `.score()`, this generates classes for a test dataset, but does take a set of labels. You will use this method in classifying any future unlabelled dataset you wish to classify.\n",
    "\n",
    "**The predictions are recorded within an array using the script below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = ann_clf.predict(test_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look at `predictions`, however, you'll find just 0s and 1s, as these relate to the outputs generated by `LabelEncoder` earlier. What we need to do is convert these back to the original label data. We do this using the `le.inverse_transform` command. We then convert this into a list, and eventually into a Dataframe for easier manipulation. \n",
    "\n",
    "**Look at the code below and execute it to produce an ordered list of predicted classifications for the `test_d` dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = pd.DataFrame(list(le.inverse_transform(predictions)))\n",
    "print(predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have the predictions, we will explore how the predictions vary with the data using the `sklearn.metrics` library. This toolkit provides a range of measures relating to the predictive power of your classifier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A nice and simple exploratory tool is a confusion matrix. This describes the the extents to which each class was correctly and incorrectly classified. It is generated using the `confusion_matrix()` method, which takes the correct and predicted results. These can then be nicely visualised using the `matplotlib` matrix plotting functions.\n",
    "\n",
    "**First, calcuating the accuracy score using the `metrics.accuracy_score` function.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "print(\"Classifcation accuracy: \")\n",
    "print(metrics.accuracy_score(test_lab, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Second, create the matrix and inspect it - what can you derive from this?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = metrics.confusion_matrix(test_lab, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now, create the confusion matrix. Where is the majority of the error in the classification?** You can read more about the confusion matrix [here](http://scikit-learn.org/stable/modules/model_evaluation.html#confusion-matrix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(confusion_matrix)\n",
    "plt.title('Confusion matrix')\n",
    "plt.colorbar()\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While these measures above provide an overview of the classifier quality, other measures made available in the `metrics` toolset enable a more detailed understanding. Many of the most important measures can be extracted using the `classification_report()` method, which again takes correct labels and compares them against the classifier predictions.\n",
    "\n",
    "These metrics, called `precision`, `recall` and `f1`, all measure how well a classifier does in predicting each class relative to how often it is incorrect. As such, it allows us to identify the prediction classes where it performs well and where it performs poorly.\n",
    "\n",
    "**Before executing the code below, look at the [documentation](http://scikit-learn.org/stable/modules/model_evaluation.html#the-scoring-parameter-defining-model-evaluation-rules) here to gain a little understanding of these measures**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification results: \")\n",
    "print(metrics.classification_report(test_lab, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above result provides a lot of insights into the classification. For example:\n",
    "\n",
    "1. How many indivuduals are correctly classified regardless of their actual class? (Hint: accuracy, 75.6%) \n",
    "1. How many individuals with incomes over 50K are correctly recognised by the classifier? (Hint: recall of Class 1, 60%)\n",
    "1. Out of all individuals that are classified as earning over 50K, how many of them actually have an income higher than 50K? (Hint: precision of Class 1, 50%)\n",
    "1. What is the F1-score of this classifier? (The macro average F1-score is 0.69)\n",
    "\n",
    "Next, we will try other classifiers to see whether we can get a higher classification accuracy or F1-score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will use **LogisticRegression**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state_lr = 200\n",
    "lr_clf = LogisticRegression(random_state=random_state_lr).fit(train_d, train_lab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of this classifier on the train and test data is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The accuracy of this classifier on the train data is:{}\".format(lr_clf.score(train_d, train_lab)))\n",
    "print(\"The accuracy of this classifier on the test data is:{}\".format(lr_clf.score(test_d, test_lab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = lr_clf.predict(test_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = metrics.confusion_matrix(test_lab, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(confusion_matrix)\n",
    "plt.title('Confusion matrix')\n",
    "plt.colorbar()\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (metrics.classification_report(test_lab, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Can you compare the accuracy and F1-score of this classifer against the ANN?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will use the `DecisionTreeClassifier`.\n",
    "\n",
    "Before you start, check the [documentation](http://scikit-learn.org/stable/modules/tree.html#classification) here.\n",
    "\n",
    "**Now create a Decision Tree Classifier for the same scenario and datasets used earlier** (you don't need to recreate the datasets, they are good to go already). We've given you the library import code below to get you started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_clf = DecisionTreeClassifier()\n",
    "tree_clf.fit(train_d, train_lab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of this classifier on the train and test data is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The accuracy of this classifier on the train data is:{}\".format(tree_clf.score(train_d, train_lab)))\n",
    "print(\"The accuracy of this classifier on the test data is:{}\".format(tree_clf.score(test_d, test_lab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy on the test data is significantly much better than that on the train data. This indicates potential overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = tree_clf.predict(test_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = metrics.confusion_matrix(test_lab, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(confusion_matrix)\n",
    "plt.title('Confusion matrix')\n",
    "plt.colorbar()\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (metrics.classification_report(test_lab, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Can you compare the accuracy and F1-score of this classifer against the ANN?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will move forward to use a Random Forest classifier.\n",
    "\n",
    "To start, read up on the `scikit` Random Forest method [here](http://scikit-learn.org/stable/modules/ensemble.html#forests-of-randomized-trees).\n",
    "\n",
    "**How well does this model perform relative to the Decision Tree and ANN?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state_RF = 200\n",
    "forest_clf = RandomForestClassifier(random_state = random_state_RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_clf.fit(train_d,train_lab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of this classifier on the train and test data is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The accuracy of this classifier on the train data is:{}\".format(forest_clf.score(train_d, train_lab)))\n",
    "print(\"The accuracy of this classifier on the test data is:{}\".format(forest_clf.score(test_d, test_lab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can compare this accuracy with the decision tree. The random forest classifer has less of the overfitting issue than the decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = forest_clf.predict(test_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = metrics.confusion_matrix(test_lab, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(confusion_matrix)\n",
    "plt.title('Confusion matrix')\n",
    "plt.colorbar()\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (metrics.classification_report(test_lab, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning for random forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learinng, hyperparameters are model settings that control the learning process (e.g. number of trees in a random forest). They should be distinguished from **parameters** that are automatically learnt in the model training process (e.g. splits in a tree of a random forest).\n",
    "\n",
    "Hyperparameters need to be predefined before model training. Normally, the packages that we use provide default value for hyperparameters. For example, if we create a `RandomForestClassifier` without specifying n_estimator, the number of trees (aka `n_estimators`) is set as 100 by default. More details can be found [here](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html).\n",
    "\n",
    "How does the number of trees influence the performance and computation of the random forest? Generally, the more trees, the smaller variance in the classification result and the higher computation cost (e.g. memory and time). The computation time is nearly a linear function of the number of trees.\n",
    "\n",
    "Here, we are interested to test whether we can find a better value for the number of trees. We would like to test a range of values, namely 50, 100, 200, 300, 400. Again, choosing these values is kind of subjective.\n",
    "\n",
    "We will use the grid search and 5-fold cross validation to tune this hyperparameter.\n",
    "\n",
    "An illustration of k-fold cross validation is as follows:\n",
    "\n",
    "![](https://github.com/huanfachen/Spatial_Data_Science/raw/main/Images/grid_search_cross_validation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we look at how to do cross validation in `sklearn`. There are different ways to do this in `sklearn`, and one of these is calling the `cross_val_score` function, which evaluates a score using cross validation. The documentation is [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html).\n",
    "\n",
    "Here we focus on the accuracy score. So, run the following code to use cross validation for a random forest classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# number of fold as 5\n",
    "cv_fold=5\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "# call the cross_val_score function\n",
    "scores = cross_val_score(clf, train_d, train_lab, cv=cv_fold)\n",
    "# note that this is an array\n",
    "print(scores) \n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result contains a list of five accuracy scores, each of which is generated by training a classifier on four folds and testing the classifier on the other fold. Then, the avearge and standard deviation of the accuracy scores is calculated and printed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use the 5-fold cross validation and grid search to tune the `n_estimators` and `max_depth` of a random forest. Note that the grid search is not limited to one or two hyperparamters. It works with any number of hyperparameters.\n",
    "\n",
    "Grid search is an exhaustive search procedure over specified hyperparameter values for a model.\n",
    "\n",
    "![](https://929687.smushcdn.com/2633864/wp-content/uploads/2021/03/hyperparameter_tuning_cv_grid_search-e1615719602429.png?lossy=1&strip=1&webp=1)\n",
    "\n",
    "[Image Credit](https://929687.smushcdn.com/2633864/wp-content/uploads/2021/03/hyperparameter_tuning_cv_grid_search-e1615719602429.png?lossy=1&strip=1&webp=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the **GridSearchCV** function from the `sklearn` library. This function integrates the grid search and cross validation procedure. Its documentation is [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html).\n",
    "\n",
    "Run the following code to do GridSearchCV. This process may take several minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# values of max_depth. 6 values ranging from 10 to 100 - what is the step length here?\n",
    "list_max_depth = [int(x) for x in np.linspace(10, 110, num = 6)]\n",
    "\n",
    "# values of n_estimators\n",
    "list_n_estimators = [50, 100, 200, 300, 400]\n",
    "# create a grid of the two hyperparameters\n",
    "grid_hyperparameters = {'n_estimators':list_n_estimators,\n",
    "                       'max_depth': list_max_depth}\n",
    "\n",
    "random_state_rf = 300\n",
    "\n",
    "rf = RandomForestClassifier(random_state_rf)\n",
    "\n",
    "clf = model_selection.GridSearchCV(rf, grid_hyperparameters)\n",
    "\n",
    "clf.fit(train_d, train_lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can query the best parameter value and its accuracy score\n",
    "print (\"The best parameter value is: \")\n",
    "print (clf.best_params_)\n",
    "print (\"The best score is: \")\n",
    "print (clf.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On my computer, it says that the best hypterparameters is n_estimators=**??** and max_depth=**??**. \n",
    "\n",
    "Note that this might be subject to randomisation, which means you could get a different result if you change the value of **random_state_rf**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some questions regarding the above grid search. Give it a try!\n",
    "\n",
    "- How many combinations of (n_estimators, max_depth) are tested in the grid search?\n",
    "\n",
    "```\n",
    "#combination = #n_estimators * #max_depth. In the example above, there are 6 values of max_depth and 5 values of max_depth, so #combination=6 * 5 = 30.\n",
    "```\n",
    "\n",
    "- What is the average run time of each combination?\n",
    "\n",
    "```\n",
    "The avrage run time of each combination = total run time / #combination.\n",
    "```\n",
    "\n",
    "- How many random forest models are built in the grid search? (Assuming that 5-fold CV is used here)\n",
    "\n",
    "```\n",
    "#random_forest_model  = #combination * #CV. #CW is the number of folds in CV.\n",
    "```\n",
    "\n",
    "\n",
    "- The value of random_state of RandomForestClassifier will lead to different models and predictive accuracy. Can you use the grid search to find the optimal random_state value?\n",
    "\n",
    "```\n",
    "No. random_state is not a hyperparameter and should not be tuned. We use a random vaue of random_state to make sure the results are reproducible. In real world application, the random_state is not fixed.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid search is a good way to tune the hyperparameters when there is only a few combinations to search. However, when there are hundreds or thousands of combinations, the exhaustive search method would be inefficient. In the later lectures and workshops, we will learn more advanced methods for tuning the hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In Conclusion..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this workshop, we have practiced several algorithms for classification and different techniques for building a classification models (e.g. splitting training and testing data, cross validation, grid search, hyperparameter tuning). Please be aware that these techniques are not restricted to classification models. They are widely applicable for supervised learning tasks, e.g. regression.\n",
    "\n",
    "There are a few challenges that I want you to explore. Go on, give at least one of them a go.\n",
    "\n",
    "- Try to use the permutation feature importance from the Week-6 workshop to understand the relative importance of these variables for predicting income levels.\n",
    "- Try to compare the results of different classification algorithms using different metrics (e.g. accuracy, precision, recall, F1). Which algorithm is the best?\n",
    "- Use these classification algorithms in another dataset (with more than two classes of the dependent variable) and compare the performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
