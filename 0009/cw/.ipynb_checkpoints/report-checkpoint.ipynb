{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0197f675-27bf-42a0-aabd-deab6a8fce40",
   "metadata": {},
   "source": [
    "# Prediction and Spatial Distribution of Arrested Individual Characteristics in SQF, New York City"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1341310e-a2bf-4516-b12f-761604826808",
   "metadata": {},
   "source": [
    "Maidi Xu, 20063089"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0289655-e0ec-408c-ba3b-2d06dd6331ee",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "This research aims to analyze the data collected by the stop-question-and-frisk program in New York. In the data given by each police precinct throughout 2020, each column represents the characteristics of the individual and the boolean output of whether it is stopped question or frisk. At this point, the research direction will be defined by whether it can predict the characteristics of arrested individuals and the distribution of people who are arrested in New York City.\n",
    "\n",
    "The output of the value point is: For predicting the characteristics of arrested individuals in various branches of New York, under the premise that there is no ethical problem entirely based on data, are there specific characteristics that are more likely to be stopped arrested? This can effectively reflect the social situation of the city. And the distribution of the arrested groups can be analyzed in combination with other spatial factors, and a reasonable argument can be given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac0f1ad-25cd-4426-9ae3-b49a6efe14ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install sphinxcontrib-bibtex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5cc880f-4a7c-412c-9421-01fa34188133",
   "metadata": {},
   "outputs": [],
   "source": [
    "extensions = ['sphinxcontrib.bibtex']\n",
    "bibtex_bibfiles = ['refs.bib']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793d6dab-d76e-4100-8122-74ea95973ee8",
   "metadata": {},
   "source": [
    "See :cite:t:`1987:nelson` for an introduction to non-standard analysis.\n",
    "Non-standard analysis is fun :cite:p:`1987:nelson`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69da6c5b-085e-4be2-bb06-0b7c28d293ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "now = datetime.datetime.now()\n",
    "print(\"Last executed: \" + now.strftime(\"%Y-%m-%d %H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f332a9c-adb9-4313-a91f-1ea2b1c01ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_rows', 300) # specifies number of rows to show\n",
    "pd.options.display.float_format = '{:40,.4f}'.format # specifies default number format to 4 decimal places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bdc83e-06e5-435f-a495-f7a1c033e74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data=pd.read_csv('sqf-2020.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b48468-df63-44fc-b2a1-8ff429704a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8685ce-0801-431e-b2c9-d90ba4c829d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd535a4-4f0b-4235-beb0-dd99fbc36e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = original_data[['STOP_FRISK_DATE', 'STOP_FRISK_TIME','DAY2','STOP_DURATION_MINUTES','OFFICER_EXPLAINED_STOP_FLAG','FRISKED_FLAG','SEARCHED_FLAG','ASK_FOR_CONSENT_FLG','SUSPECT_ARRESTED_FLAG','WEAPON_FOUND_FLAG','DEMEANOR_OF_PERSON_STOPPED','SUSPECT_REPORTED_AGE','SUSPECT_SEX','SUSPECT_RACE_DESCRIPTION','SUSPECT_BODY_BUILD_TYPE','STOP_LOCATION_X','STOP_LOCATION_Y','STOP_LOCATION_BORO_NAME']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f49c76c-b3f6-439f-a827-00e0cd0837f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b89b00b-e886-4b31-9f20-833fd349cc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "gdf = gpd.read_file('geo_export_1e43a12b-2473-4922-a8f0-580cd10da982.shp')\n",
    "gdf.set_crs(epsg=4326, inplace=True)\n",
    "gdf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cdd4cc-21d2-42d3-a5c1-1a94ef1fbaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598e11ba-407d-4062-928a-0ac0a64c815f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea02e006-a876-4def-b0e8-f12d0ee30d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(data.loc[data['STOP_LOCATION_X']==0].index, inplace=True)\n",
    "data.drop(data.loc[data['STOP_LOCATION_Y']==0].index, inplace=True)\n",
    "data_geo = gpd.GeoDataFrame(\n",
    "    data, geometry=gpd.points_from_xy(data.STOP_LOCATION_X , data.STOP_LOCATION_Y),crs='EPSG:2908'\n",
    ")\n",
    "data_geo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa580dd8-9fd8-4b6c-82ba-705b81bf47d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_geo= data_geo.to_crs(\"epsg:4326\")\n",
    "data_geo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9856c8-bc5f-4f56-805e-3e619cd73065",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_geo['lon'] = data_geo.geometry.x\n",
    "data_geo['lat'] = data_geo.geometry.y\n",
    "data_geo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3b9869-1df6-4a2a-a35b-fee4a60afc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_geo.drop(data_geo.loc[data_geo['lon'] < -75].index, inplace=True)\n",
    "data_geo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16be650-6be0-4f7b-b718-b0e9249acb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_geo.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcb2396-5b5a-444b-b042-a9337b9b8d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_geo.dropna()\n",
    "data_geo.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d40a9d6-2c89-42ce-a810-137cfdc59ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "arrested = data_geo.loc[data_geo['SUSPECT_ARRESTED_FLAG'] == 'Y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e86d18-c374-4a8a-91ac-3f2471867541",
   "metadata": {},
   "outputs": [],
   "source": [
    "arrested.dropna()\n",
    "arrested.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7278e040-d9a5-49d1-9296-5201e6dc9d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "arrested_nogeo= arrested.drop(['geometry','STOP_FRISK_DATE','STOP_FRISK_TIME','DEMEANOR_OF_PERSON_STOPPED'], 1)\n",
    "arrested_nogeo.dropna(how='all')    #to drop if all values in the row are nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493c322d-19c3-4005-8042-c92caafd7069",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "arrested_dict = arrested_nogeo.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a92a62c-a951-46e4-b2f8-cc3a314f8099",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = DictVectorizer()  # create the DictVectorizer object\n",
    "vec_array = vec.fit_transform(arrested_dict).toarray()  # execute process on the record dictionaries and transform the result into a numpy array object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fcc0a4-ff45-4e4a-9cbc-b70bf1690e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of variables in this transformed data: {}\".format(vec_array.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96931fac-0bbe-4ec4-b9e8-6919ace01360",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dde9aa6-27e1-4937-bdc0-b981a6c95075",
   "metadata": {},
   "outputs": [],
   "source": [
    "arrested_nogeo['SUSPECT_RACE_DESCRIPTION'].value_counts()[:20].plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb72feb-f25b-4b60-a48b-92f9230771a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "arrested_nogeo['SUSPECT_REPORTED_AGE'].value_counts()[:20].plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f37ae3-f893-477d-ba84-c674fbe98509",
   "metadata": {},
   "outputs": [],
   "source": [
    "arrested_blk = arrested.loc[data_geo['SUSPECT_RACE_DESCRIPTION'] == 'BLACK']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01040a87-61cb-4a95-957a-8766a6c2be72",
   "metadata": {},
   "outputs": [],
   "source": [
    "arrested_blk\n",
    "arrested_blk = gpd.GeoDataFrame(arrested_blk, crs='epsg:4326')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b762ba57-0455-4824-913c-3c72f3efd31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#arrested_blk = pd.merge(arrested_blk, gdf, left_on='geometry', right_on='geometry', how='left').reset_index()\n",
    "#arrested_blk.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3609c7d2-5404-4c83-aa34-c4298b57880c",
   "metadata": {},
   "outputs": [],
   "source": [
    "arrested_blk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28d90cd-5f50-4c51-b75b-61d3f6f19d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig = plt.figure(figsize=(9,6))\n",
    "ax.axis('off')\n",
    "\n",
    "# set aspect to equal. This is done automatically\n",
    "# when using *geopandas* plot on it's own, but not when\n",
    "# working with pyplot directly.\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "gdf.plot(ax=ax, color='white', edgecolor='lightsteelblue')\n",
    "arrested_blk.plot(ax=ax, marker='o', color='salmon', markersize=0.8, alpha = 0.2)\n",
    "\n",
    "\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6a19c7-9d9b-4293-84cc-64d75d9c9bb8",
   "metadata": {},
   "source": [
    "# classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929bb1e5-0734-4ece-9b8e-888a8d9c061b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a00561b-8ae7-47e4-8044-5e4004242dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_class = data.drop(columns=['STOP_FRISK_DATE', 'DEMEANOR_OF_PERSON_STOPPED','geometry', 'STOP_LOCATION_X', 'STOP_LOCATION_Y','STOP_LOCATION_BORO_NAME'])\n",
    "\n",
    "data_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4fe793-9905-45d5-97d6-a71874f3d437",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_class['STOP_FRISK_TIME'] = pd.to_datetime(data_class['STOP_FRISK_TIME'])\n",
    "data_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654af1d2-93d8-4fc0-9043-c22ad2befeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_class.set_index('STOP_FRISK_TIME', inplace=True)\n",
    "\n",
    "data_class.loc[data_class.between_time('00:00','06:00').any(1).index,'TIME']='early_morning'\n",
    "data_class.loc[data_class.between_time('06:00','12:00').any(1).index,'TIME']='morning'\n",
    "data_class.loc[data_class.between_time('12:00','18:00').any(1).index,'TIME']='afternoon'\n",
    "data_class.loc[data_class.between_time('18:00','23:59').any(1).index,'TIME']='night'\n",
    "\n",
    "data_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa53a012-62ff-4b8e-a8d0-e050712b7719",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_class = data_class.reset_index(drop=True)\n",
    "data_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bc357f-e6db-4ff6-b684-b0fc40c3358d",
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = data_class['DAY2'].str.contains(\"Monday|Tuesday|Wednesday|Thursday|Friday\")\n",
    "m2 = data_class['DAY2'].str.contains(\"Saturday|Sunday\")\n",
    "\n",
    "data_class['WEEK'] = np.select([m1,m2], ['weekday','weekend'], default='unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe030abd-0ee8-40d5-ab52-2e4314fbea60",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_class = data_class.drop(columns=['DAY2'])\n",
    "data_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49477edf-a669-4e9d-bc14-769cd4b50fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_class_arrest= data_class.drop(columns=['SUSPECT_ARRESTED_FLAG'])\n",
    "\n",
    "data_class_arrest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ea3248-4b43-4091-9bcc-ee4c470dc9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder() # creates the LabelEncoder object\n",
    "le.fit(['N', 'Y']) # we explicitly encode '<=50k' and '>50k' with 0 and 1, respectively\n",
    "label_y = le.transform(data_class['SUSPECT_ARRESTED_FLAG']) # runs LabelEncoder on the over50k column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6faf6dda-7df5-4e1d-a93b-76716f7fcc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "data_dict_arrest = data_class_arrest.to_dict('records')\n",
    "vec_arrest = DictVectorizer()  # create the DictVectorizer object\n",
    "vec_arrest_array = vec_arrest.fit_transform(data_dict_arrest).toarray()  # execute process on the record dictionaries and transform the result into a numpy array object\n",
    "\n",
    "vec_arrest_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a658c994-6c1f-44da-a59a-bfbbced9816e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of variables in this transformed data: {}\".format(vec_arrest_array.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a920455-7703-412f-b9e9-c4b44ec4ae41",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_arrest.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10447208-ad8f-44bc-891e-2e3b8333b342",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "random_state_split = 1024\n",
    "train_d, test_d, train_lab, test_lab = train_test_split(vec_arrest_array, label_y, random_state=random_state_split)\n",
    "\n",
    "len(train_d),len(test_d),len(train_lab),len(test_lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5b8342-0060-427e-a44f-1f7fe68650a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc744ea9-4ecb-478e-82d5-88680e199682",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "random_state_ann = 100\n",
    "# add timing script\n",
    "ann_clf = MLPClassifier(random_state = random_state_ann)  # creates the ANN classifier using the default parameters\n",
    "ann_clf.fit(train_d, train_lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cf2cd9-3eb2-40a2-914f-b5a2af80ce8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to predict the most likely class\n",
    "print(\"The predicted class of the first individual: {}\".format(ann_clf.predict(train_d[0:100, :])))\n",
    "\n",
    "# to predict the probability distribution over classes \n",
    "# the output is a list of two values, which corresponds to the probability of belonging to Class 0 and 1\n",
    "print(\"The predicted probability of the first individual: {}\".format(ann_clf.predict_proba(train_d[0:1, :])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d7569a-e3a7-4da3-9668-3006f2051c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_clf.score(test_d, test_lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b11e1ea-f499-48ae-9e94-63a202df4284",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = ann_clf.predict(test_d)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51b1639-f18d-414f-a983-5f84e6bec552",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = pd.DataFrame(list(le.inverse_transform(predictions)))\n",
    "print(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855fe8e2-4ff6-4d12-adab-7ce96c601a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "print(\"Classifcation accuracy: \")\n",
    "print(metrics.accuracy_score(test_lab, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a9decf-10bb-4b84-8de9-b234fbce7887",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = metrics.confusion_matrix(test_lab, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d4b2f8-f5ea-4172-b28e-d48070308675",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(confusion_matrix)\n",
    "plt.title('Confusion matrix')\n",
    "plt.colorbar()\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823d008e-8fd2-465e-bfc6-a63233b046d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification results: \")\n",
    "print(metrics.classification_report(test_lab, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfee4432-5725-41b8-b4a4-1f253f734567",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "random_state_RF = 200\n",
    "forest_clf = RandomForestClassifier(random_state = random_state_RF)\n",
    "\n",
    "forest_clf.fit(train_d,train_lab)\n",
    "\n",
    "print(\"The accuracy of this classifier on the train data is:{}\".format(forest_clf.score(train_d, train_lab)))\n",
    "print(\"The accuracy of this classifier on the test data is:{}\".format(forest_clf.score(test_d, test_lab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef833e3-e577-4474-922d-9f3eff4437c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = forest_clf.predict(test_d)\n",
    "confusion_matrix = metrics.confusion_matrix(test_lab, predictions)\n",
    "\n",
    "plt.matshow(confusion_matrix)\n",
    "plt.title('Confusion matrix')\n",
    "plt.colorbar()\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81688990-0ed1-47cd-a857-be5b83bd22b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (metrics.classification_report(test_lab, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c59367-12fd-4a8d-9e3e-f3118181ff87",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# number of fold as 5\n",
    "cv_fold=5\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "# call the cross_val_score function\n",
    "scores = cross_val_score(clf, train_d, train_lab, cv=cv_fold)\n",
    "# note that this is an array\n",
    "print(scores) \n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053a1c8e-baec-417d-9b86-d7fa4d26ec59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16a973a-27f4-4164-a84b-46b398e51c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# values of max_depth. 6 values ranging from 10 to 100 - what is the step length here?\n",
    "list_max_depth = [int(x) for x in np.linspace(10, 110, num = 6)]\n",
    "\n",
    "# values of n_estimators\n",
    "list_n_estimators = [50, 100, 200, 300, 400]\n",
    "# create a grid of the two hyperparameters\n",
    "grid_hyperparameters = {'n_estimators':list_n_estimators,\n",
    "                       'max_depth': list_max_depth}\n",
    "\n",
    "random_state_rf = 300\n",
    "\n",
    "rf = RandomForestClassifier(random_state_rf)\n",
    "\n",
    "clf = model_selection.GridSearchCV(rf, grid_hyperparameters)\n",
    "\n",
    "clf.fit(train_d, train_lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb75c895-218b-48ce-9c96-ee7d038a9972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can query the best parameter value and its accuracy score\n",
    "print (\"The best parameter value is: \")\n",
    "print (clf.best_params_)\n",
    "print (\"The best score is: \")\n",
    "print (clf.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eccc1be-1ca8-47fb-8d4c-b221c676eccc",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd491f44-22ab-4872-8f8c-ff490eeb26cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_data=pd.read_csv('nyc_precinct_2020pop.csv')\n",
    "pop_data = pop_data[['precinct','P1_001N']]\n",
    "\n",
    "pop_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1864ac-39aa-4dc4-9d4a-3c72282c6610",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7347ad0a-e6b8-4b8b-b8ff-0cd64a086767",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = gdf.merge(pop_data,\n",
    "                   how='left',\n",
    "                   on='precinct', \n",
    "                   copy=False)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5859ad18-62f3-4353-b287-32013feebb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "arrested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599c57c3-46f7-4325-a4fe-315ae0412b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "arrested_cluster = arrested[['geometry']]\n",
    "arrested_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ad7671-912d-4245-8300-6fe7117c9489",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import shapely.geometry\n",
    "import requests\n",
    "\n",
    "pointsInPolygon = gpd.sjoin(arrested_cluster, result, how=\"inner\", op='intersects')\n",
    "\n",
    "# Add a field with 1 as a constant value\n",
    "pointsInPolygon['const']=1\n",
    "\n",
    "# Group according to the column by which you want to aggregate data\n",
    "pointsInPolygon = pointsInPolygon.groupby(['precinct']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a36a42-6900-4933-a0a4-4bd6555963ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "pointsInPolygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e345b39e-252c-4baa-9b11-1bd2e8f6086f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pointsInPolygon = pointsInPolygon.reset_index(drop=True)\n",
    "pointsInPolygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48237c60-ebbf-470d-bb59-480cefea4b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "values = pointsInPolygon.const*1000000000 / pointsInPolygon.shape_area\n",
    "\n",
    "pointsInPolygon['density'] = values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b57f2b-199e-4e5f-a930-f97eee6f321d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pointsInPolygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20ec491-4757-488b-9ab7-d3fe8d965d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "pointsInPolygon.drop(['index_right', 'shape_area', 'shape_leng', 'const'], inplace=True, axis=1, errors='ignore')\n",
    "pointsInPolygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f296ef62-b7c0-4d38-879b-934c95aa516a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pointsInPolygon.plot.scatter(x= 'P1_001N', y='density')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cc69b9-67ab-4c20-87a9-7892751f58f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapping_clusters(labels_cluster):\n",
    "    ppd['cluster_nm'] = labels_cluster\n",
    "    ppd.plot(column='cluster_nm', categorical=True, legend=True, figsize=(12,8), cmap='Paired');\n",
    "    \n",
    "    \n",
    "\n",
    "# adapted from this tutorial: https://towardsdatascience.com/how-to-make-stunning-radar-charts-with-python-implemented-in-matplotlib-and-plotly-91e21801d8ca\n",
    "def radar_plot_cluster_centroids(df_cluster_centroid):\n",
    "    # parameters\n",
    "    # df_cluster_centroid: a dataframe with rows representing a cluster centroid and columns representing variables\n",
    "    \n",
    "    # add an additional element to both categories and restaurants thatâ€™s identical to the first item\n",
    "    # manually 'close' the line\n",
    "    categories = df_cluster_centroid.columns.values.tolist()\n",
    "    categories = [*categories, categories[0]]\n",
    "    \n",
    "    label_loc = np.linspace(start=0, stop=2 * np.pi, num=len(categories))\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.subplot(polar=True)\n",
    "    for index, row in df_cluster_centroid.iterrows():\n",
    "        centroid = row.tolist()\n",
    "        centroid = [*centroid, centroid[0]]\n",
    "        label = \"Cluster {}\".format(index)\n",
    "        plt.plot(label_loc, centroid, label=label)\n",
    "    plt.title('Cluster centroid comparison', size=20, y=1.05)\n",
    "    lines, labels = plt.thetagrids(np.degrees(label_loc), labels=categories)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06badbe6-bdf2-47b6-83ae-d394766d83b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pysal as ps\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "\n",
    "from math import ceil\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import KMeans, DBSCAN, OPTICS, AgglomerativeClustering\n",
    "from esda.adbscan import ADBSCAN\n",
    "\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "\n",
    "import spopt\n",
    "from spopt.region import MaxPHeuristic as MaxP\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import libpysal\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfded5ed-afee-4eeb-a9df-23a20d74dee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppd = gdf\n",
    "\n",
    "ppd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b5e8a0-0f7e-4ab3-b752-84c107cccdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler, MinMaxScaler\n",
    "rs = RobustScaler(quantile_range=(10.0, 90.0))\n",
    "\n",
    "normed = pointsInPolygon.copy()\n",
    "for c in pointsInPolygon.columns.values:\n",
    "    normed[c] = rs.fit_transform(pointsInPolygon[c].values.reshape(-1,1))\n",
    "    print(\"The range of {} is [{}, {}]\".format(c, normed[c].min(), normed[c].max()))\n",
    "normed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569d24d6-6201-4cde-9110-c058b33f18d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "minPts = 5 # we set minPts as normed.shape[1] + 1 \n",
    "epsilon = 0.2\n",
    "dbsc = DBSCAN(eps=epsilon, min_samples=minPts)\n",
    "dbsc.fit(normed)\n",
    "\n",
    "# We now have our DBSCAN object created, and we can extract the groups it has identified. We do this using the `.labels_` method.\n",
    "cluster_nm = dbsc.labels_\n",
    "\n",
    "mapping_clusters(cluster_nm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7262bfc8-ae81-4f12-9c9d-ea4412e643f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(dbsc.labels_).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605df93d-ee0b-4ea2-aa90-ee5f22bb6cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "metrics.silhouette_score(normed, dbsc.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e951fb-df50-4b33-afaa-9938e79d9279",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "k_cluster = 4\n",
    "random_seed = 1\n",
    "kmeans_method = KMeans(n_clusters=k_cluster,random_state=random_seed)\n",
    "kmeans_method.fit(normed)\n",
    "\n",
    "mapping_clusters(kmeans_method.labels_);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545c72e7-6dd2-4076-a691-a07c629b4a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate SSE for a range of number of cluster\n",
    "list_SSE = []\n",
    "min_k = 1\n",
    "max_k = 10\n",
    "range_k = range(min_k, max_k+1)\n",
    "for i in range_k:\n",
    "    km = KMeans(\n",
    "        n_clusters=i, init='random',\n",
    "        n_init=10, max_iter=300,\n",
    "        tol=1e-04, random_state=0\n",
    "    )\n",
    "    km.fit(normed)\n",
    "    # inertia is a concept from physics. Roughly it means SSE of clustering.\n",
    "    list_SSE.append(km.inertia_)\n",
    "\n",
    "# plot\n",
    "plt.plot(range_k, list_SSE, marker='o')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('SSE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08921714-828d-432e-964f-487793840c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_cluster = 3\n",
    "random_seed = 123\n",
    "kmeans_method = KMeans(n_clusters=k_cluster,random_state=random_seed)\n",
    "kmeans_method.fit(normed)\n",
    "\n",
    "# plotting\n",
    "mapping_clusters(kmeans_method.labels_);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90dad973-5415-4258-aa64-6bea78e5f02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dendrogram(model, **kwargs):\n",
    "    # Create linkage matrix and then plot the dendrogram\n",
    "\n",
    "    # create the counts of samples under each node\n",
    "    counts = np.zeros(model.children_.shape[0])\n",
    "    n_samples = len(model.labels_)\n",
    "    for i, merge in enumerate(model.children_):\n",
    "        current_count = 0\n",
    "        for child_idx in merge:\n",
    "            if child_idx < n_samples:\n",
    "                current_count += 1  # leaf node\n",
    "            else:\n",
    "                current_count += counts[child_idx - n_samples]\n",
    "        counts[i] = current_count\n",
    "\n",
    "    linkage_matrix = np.column_stack(\n",
    "        [model.children_, model.distances_, counts]\n",
    "    ).astype(float)\n",
    "\n",
    "    # Plot the corresponding dendrogram\n",
    "    dendrogram(linkage_matrix, leaf_rotation=90., **kwargs)\n",
    "    \n",
    "agg_cluster = AgglomerativeClustering(distance_threshold=0, n_clusters=None).fit(normed)\n",
    "ax = plt.gca()\n",
    "plt.title(\"Hierarchical Clustering Dendrogram\")\n",
    "# plot the top three levels of the dendrogram\n",
    "plot_dendrogram(agg_cluster, truncate_mode=\"level\", p=3)\n",
    "plt.xlabel(\"Number of points in node (or index of point if no parenthesis).\")\n",
    "plt.ylabel('distance')\n",
    "plt.hlines(10.7, ax.get_xlim()[0], ax.get_xlim()[1], linestyle='dashed', color='r')\n",
    "plt.hlines(14.8, ax.get_xlim()[0], ax.get_xlim()[1], linestyle='dashed', color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8306f01f-ae48-4456-b89e-ceea2f020516",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_cluster = AgglomerativeClustering(distance_threshold=None, n_clusters=4).fit(normed)\n",
    "mapping_clusters(agg_cluster.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbb7535-fa9c-4485-85e0-34f630a9b7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(agg_cluster.labels_).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91bbebe-e3e3-42e6-b4e6-5a6b72dbfe6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c69c4a8-8265-416c-a1b1-f3c70308ea3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
