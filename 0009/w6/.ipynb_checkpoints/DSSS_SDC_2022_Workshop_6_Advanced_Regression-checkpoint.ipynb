{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"float:left\">\n",
    "    <h1 style=\"width:600px\">Workshop 6: Advanced Regression</h1>\n",
    "    <h3 style=\"width:600px\">CASA0006: Data Science for Spatial Systems</h3>\n",
    "    <h3 style=\"width:600px\">CASA0009: Spatial Data Capture, Storage and Analysis</h3>\n",
    "    <h3 style=\"width:600px\">Author: Huanfa Chen</h3>\n",
    "</div>\n",
    "<div style=\"float:right\"><img width=\"100\" src=\"https://github.com/jreades/i2p/raw/master/img/casa_logo.jpg\" /></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this workshop, we will be working through some examples of how we can use regression methods to understand the relationship between the bike rental data and ambient and seasonal variables.\n",
    "\n",
    "This workshop will cover **linear regression**, **VIF**, **Lasso**, **CART**, **random forest**, and **GBDT**.\n",
    "\n",
    "Let's first import the relevant libraries. \n",
    "\n",
    "* `pandas` for data import and handling;\n",
    "* `matplotlib`;\n",
    "* `numpy`;\n",
    "* `sklearn`;\n",
    "* `statsmodels` for linear regression and VIF.\n",
    "\n",
    "**Run the script below to get started.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import statsmodels.api as sm\n",
    "\n",
    "pd.set_option('display.max_rows', 300) # specifies number of rows to show\n",
    "pd.options.display.float_format = '{:40,.4f}'.format # specifies default number format to 4 decimal places\n",
    "plt.style.use('ggplot') # specifies that graphs should use ggplot styling\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset you will use relates to daily counts of rented bicycles from the bicycle rental company Capital-Bikeshare in Washington D.C., along with weather and seasonal information. The goal here is to predict how many bikes will be rented depending on the weather and the day. The original data can be downloaded from the [UCI Machine Learning Repository](http://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset).\n",
    "\n",
    "The dataset used in this workshop has been slightly processed by Christoph Molnar using the processing R-script from this [Github repository](https://github.com/christophM/interpretable-ml-book/blob/master/R/get-bike-sharing-dataset.R). Here, the dataset is provided as a csv file on Moodle.\n",
    "\n",
    "Here is a list of the variables in the dataset:\n",
    "\n",
    "- Count of bicycles including both casual and registered users. The count is used as the response in the regression task.\n",
    "- Indicator of the season, either spring, summer, fall or winter.\n",
    "- Indicator whether the day was a holiday or not.\n",
    "- The year: either 2011 or 2012.\n",
    "- Number of days since the 01.01.2011 (the first day in the dataset). This predictor was introduced to take account of the trend over time.\n",
    "- Indicator whether the day was a working day or weekend.\n",
    "- The weather situation on that day. One of:\n",
    "  - **'GOOD'**: including clear, few clouds, partly cloudy, cloudy\n",
    "  - **'MISTY'**: including mist + clouds, mist + broken clouds, mist + few clouds, mist\n",
    "  - **'RAIN/SNOW/STORM'**: including light snow, light rain + thunderstorm + scattered clouds, light rain + scattered clouds, heavy rain + ice pallets + thunderstorm + mist, snow + mist\n",
    "- Temperature in degrees Celsius.\n",
    "- Relative humidity in percent (0 to 100).\n",
    "- Wind speed in km/h.\n",
    "\n",
    "Based on what you know from using Pandas over the last few weeks, import the Boston housing dataset as a Pandas dataframe (call it `bike_rental`), inspect the data, calculate summary statistics on all attributes, and general simple plots of pairs of attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_rental = pd.read_csv('https://raw.githubusercontent.com/huanfachen/Spatial_Data_Science/main/Dataset/daily_count_bike_rental.csv')\n",
    "# drop the year variable as it is not useful\n",
    "bike_rental = bike_rental.drop(['yr'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 731 entries, 0 to 730\n",
      "Data columns (total 11 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   cnt              731 non-null    int64  \n",
      " 1   season           731 non-null    object \n",
      " 2   mnth             731 non-null    object \n",
      " 3   holiday          731 non-null    object \n",
      " 4   weekday          731 non-null    object \n",
      " 5   workingday       731 non-null    object \n",
      " 6   weathersit       731 non-null    object \n",
      " 7   temp             731 non-null    float64\n",
      " 8   hum              731 non-null    float64\n",
      " 9   windspeed        731 non-null    float64\n",
      " 10  days_since_2011  731 non-null    int64  \n",
      "dtypes: float64(3), int64(2), object(6)\n",
      "memory usage: 62.9+ KB\n"
     ]
    }
   ],
   "source": [
    "bike_rental.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cnt</th>\n",
       "      <th>season</th>\n",
       "      <th>mnth</th>\n",
       "      <th>holiday</th>\n",
       "      <th>weekday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weathersit</th>\n",
       "      <th>temp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>days_since_2011</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>985</td>\n",
       "      <td>SPRING</td>\n",
       "      <td>JAN</td>\n",
       "      <td>NO HOLIDAY</td>\n",
       "      <td>SAT</td>\n",
       "      <td>NO WORKING DAY</td>\n",
       "      <td>MISTY</td>\n",
       "      <td>8.1758</td>\n",
       "      <td>80.5833</td>\n",
       "      <td>10.7499</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>801</td>\n",
       "      <td>SPRING</td>\n",
       "      <td>JAN</td>\n",
       "      <td>NO HOLIDAY</td>\n",
       "      <td>SUN</td>\n",
       "      <td>NO WORKING DAY</td>\n",
       "      <td>MISTY</td>\n",
       "      <td>9.0835</td>\n",
       "      <td>69.6087</td>\n",
       "      <td>16.6521</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1349</td>\n",
       "      <td>SPRING</td>\n",
       "      <td>JAN</td>\n",
       "      <td>NO HOLIDAY</td>\n",
       "      <td>MON</td>\n",
       "      <td>WORKING DAY</td>\n",
       "      <td>GOOD</td>\n",
       "      <td>1.2291</td>\n",
       "      <td>43.7273</td>\n",
       "      <td>16.6367</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1562</td>\n",
       "      <td>SPRING</td>\n",
       "      <td>JAN</td>\n",
       "      <td>NO HOLIDAY</td>\n",
       "      <td>TUE</td>\n",
       "      <td>WORKING DAY</td>\n",
       "      <td>GOOD</td>\n",
       "      <td>1.4000</td>\n",
       "      <td>59.0435</td>\n",
       "      <td>10.7398</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1600</td>\n",
       "      <td>SPRING</td>\n",
       "      <td>JAN</td>\n",
       "      <td>NO HOLIDAY</td>\n",
       "      <td>WED</td>\n",
       "      <td>WORKING DAY</td>\n",
       "      <td>GOOD</td>\n",
       "      <td>2.6670</td>\n",
       "      <td>43.6957</td>\n",
       "      <td>12.5223</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cnt  season mnth     holiday weekday      workingday weathersit  \\\n",
       "0   985  SPRING  JAN  NO HOLIDAY     SAT  NO WORKING DAY      MISTY   \n",
       "1   801  SPRING  JAN  NO HOLIDAY     SUN  NO WORKING DAY      MISTY   \n",
       "2  1349  SPRING  JAN  NO HOLIDAY     MON     WORKING DAY       GOOD   \n",
       "3  1562  SPRING  JAN  NO HOLIDAY     TUE     WORKING DAY       GOOD   \n",
       "4  1600  SPRING  JAN  NO HOLIDAY     WED     WORKING DAY       GOOD   \n",
       "\n",
       "                                      temp  \\\n",
       "0                                   8.1758   \n",
       "1                                   9.0835   \n",
       "2                                   1.2291   \n",
       "3                                   1.4000   \n",
       "4                                   2.6670   \n",
       "\n",
       "                                       hum  \\\n",
       "0                                  80.5833   \n",
       "1                                  69.6087   \n",
       "2                                  43.7273   \n",
       "3                                  59.0435   \n",
       "4                                  43.6957   \n",
       "\n",
       "                                 windspeed  days_since_2011  \n",
       "0                                  10.7499                0  \n",
       "1                                  16.6521                1  \n",
       "2                                  16.6367                2  \n",
       "3                                  10.7398                3  \n",
       "4                                  12.5223                4  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bike_rental.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `bike_rental`, there are two data types: categorical (aka `object`), and numerical (including `int64` and `float64`). \n",
    "\n",
    "Before undertaking regression, several steps should be done, which include:\n",
    "- Converting categorical variables into dummy variables;\n",
    "- Split the data into training and testing sets;\n",
    "- For linear regression, we should deal with multicollinearity (and removing some variables if necessary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting categorical variables\n",
    "First, we need to convert categorical variables into dummy/indicator variables, using `One-Hot Encoding`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_rentail_numeric = pd.get_dummies(bike_rental)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 731 entries, 0 to 730\n",
      "Data columns (total 35 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   cnt                         731 non-null    int64  \n",
      " 1   temp                        731 non-null    float64\n",
      " 2   hum                         731 non-null    float64\n",
      " 3   windspeed                   731 non-null    float64\n",
      " 4   days_since_2011             731 non-null    int64  \n",
      " 5   season_FALL                 731 non-null    uint8  \n",
      " 6   season_SPRING               731 non-null    uint8  \n",
      " 7   season_SUMMER               731 non-null    uint8  \n",
      " 8   season_WINTER               731 non-null    uint8  \n",
      " 9   mnth_APR                    731 non-null    uint8  \n",
      " 10  mnth_AUG                    731 non-null    uint8  \n",
      " 11  mnth_DEZ                    731 non-null    uint8  \n",
      " 12  mnth_FEB                    731 non-null    uint8  \n",
      " 13  mnth_JAN                    731 non-null    uint8  \n",
      " 14  mnth_JUL                    731 non-null    uint8  \n",
      " 15  mnth_JUN                    731 non-null    uint8  \n",
      " 16  mnth_MAR                    731 non-null    uint8  \n",
      " 17  mnth_MAY                    731 non-null    uint8  \n",
      " 18  mnth_NOV                    731 non-null    uint8  \n",
      " 19  mnth_OKT                    731 non-null    uint8  \n",
      " 20  mnth_SEP                    731 non-null    uint8  \n",
      " 21  holiday_HOLIDAY             731 non-null    uint8  \n",
      " 22  holiday_NO HOLIDAY          731 non-null    uint8  \n",
      " 23  weekday_FRI                 731 non-null    uint8  \n",
      " 24  weekday_MON                 731 non-null    uint8  \n",
      " 25  weekday_SAT                 731 non-null    uint8  \n",
      " 26  weekday_SUN                 731 non-null    uint8  \n",
      " 27  weekday_THU                 731 non-null    uint8  \n",
      " 28  weekday_TUE                 731 non-null    uint8  \n",
      " 29  weekday_WED                 731 non-null    uint8  \n",
      " 30  workingday_NO WORKING DAY   731 non-null    uint8  \n",
      " 31  workingday_WORKING DAY      731 non-null    uint8  \n",
      " 32  weathersit_GOOD             731 non-null    uint8  \n",
      " 33  weathersit_MISTY            731 non-null    uint8  \n",
      " 34  weathersit_RAIN/SNOW/STORM  731 non-null    uint8  \n",
      "dtypes: float64(3), int64(2), uint8(30)\n",
      "memory usage: 50.1 KB\n"
     ]
    }
   ],
   "source": [
    "# check out the new dataFrame\n",
    "bike_rentail_numeric.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that, a cateogircal variable of K categories or levels, usually enters a regression as a sequence of K-1 dummy variables. The level that is left out becomes the reference level, and this is important for interpreting the regression model.\n",
    "\n",
    "Here we manually choose the reference level for each categorical variable and exclude them from the DataFrame. You can change the reference levels if you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 731 entries, 0 to 730\n",
      "Data columns (total 29 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   cnt                         731 non-null    int64  \n",
      " 1   temp                        731 non-null    float64\n",
      " 2   hum                         731 non-null    float64\n",
      " 3   windspeed                   731 non-null    float64\n",
      " 4   days_since_2011             731 non-null    int64  \n",
      " 5   season_FALL                 731 non-null    uint8  \n",
      " 6   season_SUMMER               731 non-null    uint8  \n",
      " 7   season_WINTER               731 non-null    uint8  \n",
      " 8   mnth_APR                    731 non-null    uint8  \n",
      " 9   mnth_AUG                    731 non-null    uint8  \n",
      " 10  mnth_DEZ                    731 non-null    uint8  \n",
      " 11  mnth_FEB                    731 non-null    uint8  \n",
      " 12  mnth_JUL                    731 non-null    uint8  \n",
      " 13  mnth_JUN                    731 non-null    uint8  \n",
      " 14  mnth_MAR                    731 non-null    uint8  \n",
      " 15  mnth_MAY                    731 non-null    uint8  \n",
      " 16  mnth_NOV                    731 non-null    uint8  \n",
      " 17  mnth_OKT                    731 non-null    uint8  \n",
      " 18  mnth_SEP                    731 non-null    uint8  \n",
      " 19  holiday_HOLIDAY             731 non-null    uint8  \n",
      " 20  weekday_FRI                 731 non-null    uint8  \n",
      " 21  weekday_SAT                 731 non-null    uint8  \n",
      " 22  weekday_SUN                 731 non-null    uint8  \n",
      " 23  weekday_THU                 731 non-null    uint8  \n",
      " 24  weekday_TUE                 731 non-null    uint8  \n",
      " 25  weekday_WED                 731 non-null    uint8  \n",
      " 26  workingday_NO WORKING DAY   731 non-null    uint8  \n",
      " 27  weathersit_MISTY            731 non-null    uint8  \n",
      " 28  weathersit_RAIN/SNOW/STORM  731 non-null    uint8  \n",
      "dtypes: float64(3), int64(2), uint8(24)\n",
      "memory usage: 45.8 KB\n"
     ]
    }
   ],
   "source": [
    "bike_rental_final = bike_rentail_numeric.drop(['season_SPRING', 'mnth_JAN', 'holiday_NO HOLIDAY', 'weekday_MON', 'workingday_WORKING DAY', 'weathersit_GOOD'], axis=1)\n",
    "\n",
    "# double check the result\n",
    "bike_rental_final.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting data into random train and test subsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, `train_test_split` will split the data according to a 75:25 split. Other proportions can be specified, check out the [documentation](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.train_test_split.html) for details.\n",
    "\n",
    "Remember that the split should be random in order to avoid selection bias. Here, we set random_state=100 to guarantee reproducibility.\n",
    "\n",
    "From the documentation:\n",
    "\n",
    "The first argument of this function:\n",
    "\n",
    "```\n",
    "*arrays: sequence of indexables with same length / shape[0]\n",
    "Allowed inputs are lists, numpy arrays, scipy-sparse matrices or pandas dataframes.\n",
    "```\n",
    "\n",
    "The output of this function:\n",
    "```\n",
    "splitting: list, length=2 * len(arrays)\n",
    "List containing train-test split of inputs.\n",
    "```\n",
    "\n",
    "Here we input two dataframes (X and Y) and will get four outputs (train_x, test_x, train_y, test_y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2339148957.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_1308/2339148957.py\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    train_x, test_x, train_y, test_y = train_test_split(bike_rental_final.drop(['cnt'], axis = ??), bike_rental_final.cnt, random_state=random_state_split)\u001b[0m\n\u001b[0m                                                                                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "random_state_split = 100\n",
    "train_x, test_x, train_y, test_y = train_test_split(bike_rental_final.drop(['cnt'], axis = ??), bike_rental_final.cnt, random_state=random_state_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double check the rows and columns of the outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_x.shape)\n",
    "print(train_y.??)\n",
    "print(test_x.shape)\n",
    "print(test_y.??)\n",
    "\n",
    "# check the index of train_x and train_y - they should be identical. The index indicates which rows from the original data.\n",
    "\n",
    "print(train_x.index.identical(train_y.index))\n",
    "print(test_x.index.identical(test_y.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the regression models, we will use the training set to train the model and select hyperparameters. The testing set is only used to report the performance of the finalised model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking multicollinearity\n",
    "\n",
    "In regression, `multicollinearity` emerges when two or more variables, which are highly correlated are included in a model. It can emerge even when each pair of variables involved are not colinear. For example, think of the four dummy variables relating to seasons in the `bike_rental_numeric` dataset.  \n",
    "\n",
    "You can check the colinearity between attributes using a correlation matrix as below. Think of these two questions:\n",
    "\n",
    "- **What can you get from this plot? For example, which pair of variables are highly correlated, and can you explain it?**\n",
    "- **Can you use the correlation matrix to deal with multicolinearity, for example, by removing one of the two variables if their correlation is above 0.75?** (*Hint*: a short answer is NO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = bike_rental_final\n",
    "plt.rcParams[\"axes.grid\"] = False\n",
    "f = plt.figure(figsize=(19, 15))\n",
    "plt.matshow(df.corr(), fignum=f.number)\n",
    "plt.xticks(range(df.shape[1]), df.columns, fontsize=14, rotation=45)\n",
    "plt.yticks(range(df.shape[1]), df.columns, fontsize=14)\n",
    "cb = plt.colorbar()\n",
    "cb.ax.tick_params(labelsize=14)\n",
    "plt.title('Correlation Matrix', fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression with VIF\n",
    "\n",
    "The `Variance Inflation Factor` (VIF) is a measure of multicolinearity among predictors within a multiple regression task. It is the quotient of the variance in a model with multiple predictors by the variance of a model with a single predictor. More explanation of the theory can be found [here](https://en.wikipedia.org/wiki/Variance_inflation_factor). \n",
    "\n",
    "Steps for using VIF to deal with multicolineartiy are:\n",
    "\n",
    "1. Initialise ```L``` as the list of independent variables. (*HINT*: the response variable is not needed here)\n",
    "2. Calculate the VIF for each variable in ```L```. (*HINT*: the order of computing VIF is irrelevant).\n",
    "3. If the highest VIF is larger than the threshold, remove the variable from the list ```L```.\n",
    "4. Repeat Step 2-3, until no VIF is larger than the threshold.\n",
    "5. Return ```L```.\n",
    "\n",
    "Below is an example of using VIF on the `bike_rental` dataset. Note that the `statsmodels` package is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating VIF\n",
    "# This function is amended from: https://stackoverflow.com/a/51329496/4667568\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor \n",
    "from statsmodels.tools.tools import add_constant\n",
    "\n",
    "def drop_column_using_vif_(df, list_var_not_to_remove=None, thresh=5):\n",
    "    '''\n",
    "    Calculates VIF each feature in a pandas dataframe, and repeatedly drop the columns with the highest VIF\n",
    "    A constant must be added to variance_inflation_factor or the results will be incorrect\n",
    "\n",
    "    :param df: the pandas dataframe containing only the predictor features, not the response variable\n",
    "    :param list_var_not_to_remove: the list of variables that should not be removed even though it has a high VIF. For example, dummy (or indicator) variables represent a categorical variable with three or more categories.\n",
    "    :param thresh: the max VIF value before the feature is removed from the dataframe\n",
    "    :return: dataframe with multicollinear features removed\n",
    "    '''\n",
    "    while True:\n",
    "        # adding a constatnt item to the data\n",
    "        df_with_const = add_constant(df)\n",
    "\n",
    "        vif_df = pd.Series([variance_inflation_factor(df_with_const.values, i) \n",
    "               for i in range(df_with_const.shape[1])], name= \"VIF\",\n",
    "              index=df_with_const.columns).to_frame()\n",
    "\n",
    "        # drop the const as const should not be removed\n",
    "        vif_df = vif_df.drop('const')\n",
    "        \n",
    "        # drop the variables that should not be removed\n",
    "        if list_var_not_to_remove is not None:\n",
    "            vif_df = vif_df.drop(list_var_not_to_remove)\n",
    "            \n",
    "        print('Max VIF:', vif_df.VIF.max())\n",
    "        \n",
    "        # if the largest VIF is above the thresh, remove a variable with the largest VIF\n",
    "        if vif_df.VIF.max() > thresh:\n",
    "            # If there are multiple variables with the maximum VIF, choose the first one\n",
    "            index_to_drop = vif_df.index[vif_df.VIF == vif_df.VIF.max()].tolist()[0]\n",
    "            print('Dropping: {}'.format(index_to_drop))\n",
    "            df = df.drop(columns = index_to_drop)\n",
    "        else:\n",
    "            # No VIF is above threshold. Exit the loop\n",
    "            break\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using VIF on the bike_rental dataset\n",
    "\n",
    "train_x_VIF = drop_column_using_vif_(train_x, thresh=5)\n",
    "print(\"The columns remaining after VIF selection are:\")\n",
    "print(train_x_VIF.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a linear regression model\n",
    "\n",
    "Now we can fit a linear regression model after dealing with the multicolinearity, and we will do it using the sklearn package. \n",
    "\n",
    "Just like clustering analysis in `sklearn`, we will run the regression model using the `.fit()` function. Make sure you get the variables the right way around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X=train_x_VIF, y=??)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've created our fit, it's time to look at the structure of the model and how well it fits the data.\n",
    "\n",
    "There are a number of ways to do this. In the first instance, as with any linear regression model, we want to get a grip on the coefficients and intercept of the model. Helpfully, these two functions are built into the LinearRegression object. **Consult the documentation and see if you can find out how to extract these.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Constant term: ', lr.intercept_)\n",
    "df_coef_lr_VIF = pd.DataFrame({\"var\": train_x_VIF.columns.values, \"coef\":lr.coef_})\n",
    "print(df_coef_lr_VIF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below will generate the R2 score for us. Run the code and see how well the model performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('R2:')\n",
    "lr.score(X=??, y=train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is not easy to get a comprehensive summary of the regression model using `sklearn` package. We recommend using the `statsmodel` package, which has a nice **summary()** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a lineare regression model using statsmodel\n",
    "regressor_OLS = sm.OLS(endog=train_y, exog=sm.add_constant(train_x_VIF)).fit()\n",
    "regressor_OLS.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The R2 value of 0.811 is quite high, which means that 81.1% of the variance in the daily bike rental can be explained by the predictors used.\n",
    "\n",
    "The next step is interpreting this model. Regarding the humnidity, an increase of the humidity by 1% decreases the predicted bike rental count by 6.7672, when all other variables remain fixed. In terms of seasons, when it is fall, the predicted number of bike rental is 1120.6 higher compared to the spring season, when all other variables remain fixed. (So people like travelling by bicycle in fall more than in spring?)\n",
    "\n",
    "As a practice, can you interpret the other variables in this model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can evaluate the model performance on the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the same columns from test_x as train_x_VIF\n",
    "test_x_VIF = test_x[train_x_VIF.columns]\n",
    "\n",
    "# using the model.score function to get the R2 on the testing data\n",
    "lr.score(X=test_x_VIF, y=test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression with Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso (which means **least absolute shrinkage and selection operator**) is a regression method that performs both variable selection and regularisation (or penalty) in order to increase the prediction accuracy and interpretability of the regression model.\n",
    "\n",
    "In Lasso, the lambda (or alpha) parameter controls the strength of regularisation. If alpha=0, it is equivalent to an ordinary least square. \n",
    "\n",
    "Note that in the Lasso function provided by `sklearn`, the alpha value defaults to 1.0. But this value can be optimised.\n",
    "\n",
    "Please note that Lasso requires predictors to be normalised. This can be done using this function by setting `normalize=True`. Here, normalisation means subtracting the mean and dividing by the standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# increase the max_iter to guarantee convergence\n",
    "lasso_model = sklearn.linear_model.Lasso(max_iter=10e7, normalize=True)\n",
    "lasso_model.fit(X=train_x, y=train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_model.score(X=train_x, y=train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the Lasso model has done a great job in fitting the data relationship in the training set.\n",
    "\n",
    "We can check out the coefficients using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(lasso_model.coef_)\n",
    "df_coef_lasso = pd.DataFrame({\"var\": train_x.columns.values, \"coef\":lasso_model.coef_})\n",
    "print(df_coef_lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply the Lasso regression with a range of lambda parameters, and see how the coefficients change with the lambda value. The plot is called `LASSO Path`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import cycle\n",
    "from sklearn.linear_model import lasso_path\n",
    "\n",
    "train_x_standard = train_x/train_x.std(axis=0)\n",
    "list_alphas = [.1, .5, 1, 10, 100, 1000, 10000]\n",
    "\n",
    "# alphas_lasso, coefs_lasso, _ = lasso_path(predictors_bike_rental, response_bike_rental, alphas = [.1, .5, 1, 10, 100], fit_intercept=False)\n",
    "alphas_lasso, coefs_lasso, _ = lasso_path(train_x_standard, train_y.values.reshape(-1),alphas = list_alphas, fit_intercept=False)\n",
    "# print(alphas_lasso.shape)\n",
    "# print(coefs_lasso.shape)\n",
    "\n",
    "log_alphas_lasso = np.log10(alphas_lasso)\n",
    "for coef_l in coefs_lasso:\n",
    "    l1 = plt.plot(log_alphas_lasso, coef_l)\n",
    "\n",
    "plt.xlabel('Log(alpha)')\n",
    "plt.ylabel('coefficients')\n",
    "plt.title('Lasso Path')\n",
    "plt.axis('tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use cross validation to search for the optimal alpha value. The code below is based on [this link](https://scikit-learn.org/stable/auto_examples/linear_model/plot_lasso_model_selection.html#sphx-glr-auto-examples-linear-model-plot-lasso-model-selection-py).\n",
    "\n",
    "What is the search range of alphas? \n",
    "\n",
    "The **lassoCV** function an argument called **alphas**, which is the list of alphas where to compute the models. Its default value is None. If None, alphas are set automatically.\n",
    "\n",
    "As the automatically selected alpha values don't work well here (reason unknown), we will manually set alphas to list_alphas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "lasso_cv = LassoCV(cv=20, alphas=list_alphas).fit(train_x, train_y)\n",
    "fit_time = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ymin, ymax = np.min(lasso_cv.mse_path_), np.max(lasso_cv.mse_path_)\n",
    "# lasso = model[-1]\n",
    "plt.semilogx(lasso_cv.alphas_, lasso_cv.mse_path_, linestyle=\":\")\n",
    "plt.plot(\n",
    "    lasso_cv.alphas_,\n",
    "    lasso_cv.mse_path_.mean(axis=-1),\n",
    "    color=\"black\",\n",
    "    label=\"Average across the folds\",\n",
    "    linewidth=2,\n",
    ")\n",
    "plt.axvline(lasso_cv.alpha_, linestyle=\"--\", color=\"black\", label=\"alpha: CV estimate\")\n",
    "\n",
    "plt.ylim(ymin, ymax)\n",
    "plt.xlabel(r\"$\\alpha$\")\n",
    "plt.ylabel(\"Mean square error\")\n",
    "plt.legend()\n",
    "_ = plt.title(\n",
    "    f\"Mean square error on each fold: coordinate descent (train time: {fit_time:.2f}s)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the **alphas_** values that have been compared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lasso_cv.alphas_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal alpha value is as follows.\n",
    "\n",
    "Note that this calculation contains some randomness and the mean square area of alpha=0.1 and alpha=1 is very close. Therefore, the optimal alpha might be 0.1 or 1 and might differ in different runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lasso_cv.alpha_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rerun the lasso model using the optimal alpha value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# increase the max_iter to guarantee convergence\n",
    "lasso_model = sklearn.linear_model.Lasso(max_iter=10e7, normalize=True, alpha = lasso_cv.alpha_)\n",
    "lasso_model.fit(X=train_x, y=train_y)\n",
    "print(lasso_model.score(train_x, train_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does this model perform on the testing data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lasso_model.score(??, ??))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will use a regression tree for this dataset. Although there are several parameters in this model such as **max_depth** (corresponding to the height of a tree), but in a first attempt we just use the default values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "reg_tree = DecisionTreeRegressor(random_state=0)\n",
    "# if you don't remember the meaning of random_state, check out the lastest clustering workshop on Moodle\n",
    "reg_tree.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like the previous regressors, you can check the R2 score using the `score()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"R2 on the training data:\")\n",
    "print(reg_tree.score(X=train_x, y=??))\n",
    "print(\"R2 on the testing data:\")\n",
    "print(reg_tree.score(X=test_x, y=??))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the R2 on the testing data is much lower than that on the training data. This indicates the overfitting problem, meaning that it fits very well to the training data but fails to generalise to unseen data.\n",
    "\n",
    "CART tends to overfit, and this issue can be reduced by **ensemble** methods which combine many many trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you prefer RMSE as the performance metric, you can calculate it as follows (see the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html#sklearn.metrics.mean_squared_error)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "print(\"RMSE on the training data:\")\n",
    "print(mean_squared_error(train_y, reg_tree.predict(train_x), squared=False))\n",
    "print(\"RMSE on the testing data:\")\n",
    "print(mean_squared_error(test_y, reg_tree.predict(test_x), squared=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation of the regression tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many layers and leaf nodes are in this tree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Depth of the regression tree: {}\".format(reg_tree.get_depth()))\n",
    "print(\"Number of nodes of this tree: {}\".format(reg_tree.get_n_leaves()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this tree has over 20 layers and hundreds of leaves, it is not useful to visualise this tree. \n",
    "\n",
    "Alternatively, we will use the Permutation Feature Importance (PFI) to evaluate the relative importance of each variable.\n",
    "\n",
    "We will use the package of **rfpimp** to compute the variable importance. If this package is not installed, install it using ```pip install rfpimp```.\n",
    "\n",
    "An introduction of this package is [here](https://github.com/parrt/random-forest-importances).\n",
    "\n",
    "A good introduction to the permutation feature importance is [here](https://christophm.github.io/interpretable-ml-book/feature-importance.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install rfpimp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the feature importance should be evaluated using the testing set, as it is unused in the model training.\n",
    "\n",
    "We are interested in the top variables with a positive feature importance. These are days_since_2011, temp, hum, windspeed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rfpimp\n",
    "imp = rfpimp.importances(reg_tree, test_x, test_y) # permutation\n",
    "print(imp)\n",
    "viz = rfpimp.plot_importances(imp)\n",
    "viz.view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will use the random forest method for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "reg_random_forest = RandomForestRegressor(random_state=0)\n",
    "reg_random_forest.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the R2 score of this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"R2 on the training data:\")\n",
    "print(reg_random_forest.score(X=train_x, y=train_y))\n",
    "print(\"R2 on the testing data:\")\n",
    "print(reg_random_forest.score(X=test_x, y=test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So - what do you think about the comparison of R2 on the training and testing data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with CART, it is not useful to visualise the trees in a random forest.\n",
    "\n",
    "Calculate and plot the feature importance of the RF model.\n",
    "\n",
    "Although the feature importance value differs from the CART, the order of the top-ranking variables is the same.\n",
    "\n",
    "This indicates that these variables are indeed very important for the prediction of bike rental."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp = rfpimp.importances(reg_random_forest, test_x, test_y) # permutation\n",
    "print(imp)\n",
    "viz = rfpimp.plot_importances(??)\n",
    "viz.view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation: permutation feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GBDT and XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last but not least, we will use GBDT to model and predict the bike rental.\n",
    "\n",
    "We will use the **XGBoost** package. Although it is not part of sklearn, it has a sklearn-style interface, which is very convenient for users.\n",
    "\n",
    "If the **XGBoost** package is not installed, install it using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "random_state_xgb = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = xgb.XGBRegressor(random_state = random_state_xgb)\n",
    "\n",
    "xgb_model.fit(??, ??)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the performance of XGB regressor on the training and testing set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"R2 on the training data:\")\n",
    "print(xgb_model.score(X=train_x, y=train_y))\n",
    "print(\"R2 on the testing data:\")\n",
    "print(xgb_model.score(X=test_x, y=test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the relative feature importance in this model?\n",
    "\n",
    "Can you compare the variable importance with the CART and RF?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp = rfpimp.importances(??, ??, ??) # permutation\n",
    "print(imp)\n",
    "viz = rfpimp.plot_importances(imp)\n",
    "viz.view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have built a range of regression models. Now we can collate these models and compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of models\n",
    "list_name_models = ['LR_VIF', 'LR_LASSO', 'CART', 'RF', 'GBDT']\n",
    "# use the models from above\n",
    "list_reg_models = [lr, lasso_model, reg_tree, reg_random_forest, xgb_model]\n",
    "\n",
    "dict_models = dict()\n",
    "\n",
    "# Element of dict_models: {'LR_VIF': [train_R2, test_R2]}\n",
    "\n",
    "# Example\n",
    "# data = {'row_1': [3, 2, 1, 0], 'row_2': ['a', 'b', 'c', 'd']}\n",
    "# pd.DataFrame.from_dict(data, orient='index')\n",
    "#        0  1  2  3\n",
    "# row_1  3  2  1  0\n",
    "# row_2  a  b  c  d\n",
    "\n",
    "for name, model in zip(list_name_models, list_reg_models):\n",
    "    if name == 'LR_VIF':\n",
    "        dict_models[name] = [model.score(train_x_VIF, train_y), model.score(test_x_VIF, test_y)]\n",
    "    else:\n",
    "        dict_models[name] = [model.score(train_x, ??), model.score(test_x, ??)]\n",
    "\n",
    "# transform dict_models to dataframe\n",
    "df_models = pd.DataFrame.from_dict(dict_models, orient='index', columns=['R2_train_data', 'R2_test_data'])\n",
    "print(df_models)\n",
    "# you can then export df_models as a csv file and use it in MS Excel or Word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well done for completing this workshop on regression. You will now have a stronger understanding of how to use a range of regression methods to gain insights into your datasets.\n",
    "\n",
    "If you have time and/or interest in exploring these methods further, why not try these methods on a new dataset?\n",
    "\n",
    "There are lots of nice datasets for analysis on [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets.html) and [Kaggle](https://www.kaggle.com/datasets). Take a look, and if any of them take your fancy try create a regression model. Which data features are important? Which are not? \n",
    "\n",
    "We haven't talked about hyperparameter tuning of RF/XGBoost models. This topic will be discussed in the next weeks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credits!\n",
    "\n",
    "### Acknowledgements\n",
    "\n",
    "This workshop is partially based on a [chapter](https://christophm.github.io/interpretable-ml-book/limo.html) of the [book](https://christophm.github.io/interpretable-ml-book/) by Christoph Molnar. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
